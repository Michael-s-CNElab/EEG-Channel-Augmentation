{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\huggingface\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\preprocessor_config.json\n",
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\preprocessor_config.json\n",
      "Image processor VideoMAEImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"VideoMAEImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"processor_class\": \"GitProcessor\",\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\vocab.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\tokenizer_config.json\n",
      "Processor GitProcessor:\n",
      "- image_processor: VideoMAEImageProcessor {\n",
      "  \"crop_size\": {\n",
      "    \"height\": 224,\n",
      "    \"width\": 224\n",
      "  },\n",
      "  \"do_center_crop\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"VideoMAEImageProcessor\",\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"processor_class\": \"GitProcessor\",\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"shortest_edge\": 224\n",
      "  }\n",
      "}\n",
      "\n",
      "- tokenizer: BertTokenizerFast(name_or_path='microsoft/git-base-vatex', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"GitProcessor\"\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\config.json\n",
      "vision_config is None. initializing the GitVisionConfig with default values.\n",
      "Model config GitConfig {\n",
      "  \"_name_or_path\": \"microsoft/git-base-vatex\",\n",
      "  \"architectures\": [\n",
      "    \"GitForCausalLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 101,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"git\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_image_with_embedding\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.46.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vision_config\": {\n",
      "    \"dropout\": 0.0,\n",
      "    \"initializer_factor\": 1.0,\n",
      "    \"model_type\": \"git_vision_model\",\n",
      "    \"projection_dim\": 512\n",
      "  },\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 101,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GitForCausalLM.\n",
      "\n",
      "All the weights of GitForCausalLM were initialized from the model checkpoint at microsoft/git-base-vatex.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GitForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at C:\\Users\\user\\.cache\\huggingface\\hub\\models--microsoft--git-base-vatex\\snapshots\\623363cef8744c57438ce8d62b30bef1fbbec111\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 101,\n",
      "  \"eos_token_id\": 102,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vatex\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vatex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vision_config is None. initializing the GitVisionConfig with default values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GitConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"microsoft/git-base-vatex\",\n",
       "  \"architectures\": [\n",
       "    \"GitForCausalLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 101,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 102,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"git\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"num_image_with_embedding\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vision_config\": {\n",
       "    \"_attn_implementation_autoset\": true,\n",
       "    \"dropout\": 0.0,\n",
       "    \"initializer_factor\": 1.0,\n",
       "    \"model_type\": \"git_vision_model\",\n",
       "    \"projection_dim\": 512\n",
       "  },\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GitEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GitVisionModel were not initialized from the model checkpoint at microsoft/git-base and are newly initialized: ['git.vision_model.embeddings.class_embedding', 'git.vision_model.embeddings.patch_embedding.weight', 'git.vision_model.embeddings.position_embedding.weight', 'git.vision_model.encoder.layers.0.layer_norm1.bias', 'git.vision_model.encoder.layers.0.layer_norm1.weight', 'git.vision_model.encoder.layers.0.layer_norm2.bias', 'git.vision_model.encoder.layers.0.layer_norm2.weight', 'git.vision_model.encoder.layers.0.mlp.fc1.bias', 'git.vision_model.encoder.layers.0.mlp.fc1.weight', 'git.vision_model.encoder.layers.0.mlp.fc2.bias', 'git.vision_model.encoder.layers.0.mlp.fc2.weight', 'git.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.1.layer_norm1.bias', 'git.vision_model.encoder.layers.1.layer_norm1.weight', 'git.vision_model.encoder.layers.1.layer_norm2.bias', 'git.vision_model.encoder.layers.1.layer_norm2.weight', 'git.vision_model.encoder.layers.1.mlp.fc1.bias', 'git.vision_model.encoder.layers.1.mlp.fc1.weight', 'git.vision_model.encoder.layers.1.mlp.fc2.bias', 'git.vision_model.encoder.layers.1.mlp.fc2.weight', 'git.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.10.layer_norm1.bias', 'git.vision_model.encoder.layers.10.layer_norm1.weight', 'git.vision_model.encoder.layers.10.layer_norm2.bias', 'git.vision_model.encoder.layers.10.layer_norm2.weight', 'git.vision_model.encoder.layers.10.mlp.fc1.bias', 'git.vision_model.encoder.layers.10.mlp.fc1.weight', 'git.vision_model.encoder.layers.10.mlp.fc2.bias', 'git.vision_model.encoder.layers.10.mlp.fc2.weight', 'git.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.11.layer_norm1.bias', 'git.vision_model.encoder.layers.11.layer_norm1.weight', 'git.vision_model.encoder.layers.11.layer_norm2.bias', 'git.vision_model.encoder.layers.11.layer_norm2.weight', 'git.vision_model.encoder.layers.11.mlp.fc1.bias', 'git.vision_model.encoder.layers.11.mlp.fc1.weight', 'git.vision_model.encoder.layers.11.mlp.fc2.bias', 'git.vision_model.encoder.layers.11.mlp.fc2.weight', 'git.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.2.layer_norm1.bias', 'git.vision_model.encoder.layers.2.layer_norm1.weight', 'git.vision_model.encoder.layers.2.layer_norm2.bias', 'git.vision_model.encoder.layers.2.layer_norm2.weight', 'git.vision_model.encoder.layers.2.mlp.fc1.bias', 'git.vision_model.encoder.layers.2.mlp.fc1.weight', 'git.vision_model.encoder.layers.2.mlp.fc2.bias', 'git.vision_model.encoder.layers.2.mlp.fc2.weight', 'git.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.3.layer_norm1.bias', 'git.vision_model.encoder.layers.3.layer_norm1.weight', 'git.vision_model.encoder.layers.3.layer_norm2.bias', 'git.vision_model.encoder.layers.3.layer_norm2.weight', 'git.vision_model.encoder.layers.3.mlp.fc1.bias', 'git.vision_model.encoder.layers.3.mlp.fc1.weight', 'git.vision_model.encoder.layers.3.mlp.fc2.bias', 'git.vision_model.encoder.layers.3.mlp.fc2.weight', 'git.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.4.layer_norm1.bias', 'git.vision_model.encoder.layers.4.layer_norm1.weight', 'git.vision_model.encoder.layers.4.layer_norm2.bias', 'git.vision_model.encoder.layers.4.layer_norm2.weight', 'git.vision_model.encoder.layers.4.mlp.fc1.bias', 'git.vision_model.encoder.layers.4.mlp.fc1.weight', 'git.vision_model.encoder.layers.4.mlp.fc2.bias', 'git.vision_model.encoder.layers.4.mlp.fc2.weight', 'git.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.5.layer_norm1.bias', 'git.vision_model.encoder.layers.5.layer_norm1.weight', 'git.vision_model.encoder.layers.5.layer_norm2.bias', 'git.vision_model.encoder.layers.5.layer_norm2.weight', 'git.vision_model.encoder.layers.5.mlp.fc1.bias', 'git.vision_model.encoder.layers.5.mlp.fc1.weight', 'git.vision_model.encoder.layers.5.mlp.fc2.bias', 'git.vision_model.encoder.layers.5.mlp.fc2.weight', 'git.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.6.layer_norm1.bias', 'git.vision_model.encoder.layers.6.layer_norm1.weight', 'git.vision_model.encoder.layers.6.layer_norm2.bias', 'git.vision_model.encoder.layers.6.layer_norm2.weight', 'git.vision_model.encoder.layers.6.mlp.fc1.bias', 'git.vision_model.encoder.layers.6.mlp.fc1.weight', 'git.vision_model.encoder.layers.6.mlp.fc2.bias', 'git.vision_model.encoder.layers.6.mlp.fc2.weight', 'git.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.7.layer_norm1.bias', 'git.vision_model.encoder.layers.7.layer_norm1.weight', 'git.vision_model.encoder.layers.7.layer_norm2.bias', 'git.vision_model.encoder.layers.7.layer_norm2.weight', 'git.vision_model.encoder.layers.7.mlp.fc1.bias', 'git.vision_model.encoder.layers.7.mlp.fc1.weight', 'git.vision_model.encoder.layers.7.mlp.fc2.bias', 'git.vision_model.encoder.layers.7.mlp.fc2.weight', 'git.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.8.layer_norm1.bias', 'git.vision_model.encoder.layers.8.layer_norm1.weight', 'git.vision_model.encoder.layers.8.layer_norm2.bias', 'git.vision_model.encoder.layers.8.layer_norm2.weight', 'git.vision_model.encoder.layers.8.mlp.fc1.bias', 'git.vision_model.encoder.layers.8.mlp.fc1.weight', 'git.vision_model.encoder.layers.8.mlp.fc2.bias', 'git.vision_model.encoder.layers.8.mlp.fc2.weight', 'git.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'git.vision_model.encoder.layers.9.layer_norm1.bias', 'git.vision_model.encoder.layers.9.layer_norm1.weight', 'git.vision_model.encoder.layers.9.layer_norm2.bias', 'git.vision_model.encoder.layers.9.layer_norm2.weight', 'git.vision_model.encoder.layers.9.mlp.fc1.bias', 'git.vision_model.encoder.layers.9.mlp.fc1.weight', 'git.vision_model.encoder.layers.9.mlp.fc2.bias', 'git.vision_model.encoder.layers.9.mlp.fc2.weight', 'git.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'git.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'git.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'git.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'git.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'git.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'git.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'git.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'git.vision_model.post_layernorm.bias', 'git.vision_model.post_layernorm.weight', 'git.vision_model.pre_layrnorm.bias', 'git.vision_model.pre_layrnorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Pixel size=torch.Size([1, 3, 224, 224])\n",
      "Embedding size=torch.Size([1, 197, 768])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "Q raw=torch.Size([1, 197, 768])\n",
      "Q state=torch.Size([12, 197, 64])\n",
      "Attention Socres=torch.Size([12, 197, 197])\n",
      "GitVisionConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"_name_or_path\": \"microsoft/git-base\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dropout\": 0.0,\n",
      "  \"hidden_act\": \"quick_gelu\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"model_type\": \"git_vision_model\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"projection_dim\": 512,\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, GitVisionModel, BertModel\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n",
    "model = GitVisionModel.from_pretrained(\"microsoft/git-base\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

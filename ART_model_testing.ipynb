{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact Removal Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 應該顯示 True\n",
    "print(torch.version.cuda)         # 應該顯示 11.8\n",
    "print(torch.backends.cudnn.version())  # 確保 cuDNN 也可用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from tf_model import make_model\n",
    "from torchinfo import summary\n",
    "\n",
    "model = make_model(30, 30, N=2)\n",
    "print(summary(model, input_size=[(32, 30, 120),(32, 30, 120),(32, 120, 120),(32, 120, 120)], col_names=[\"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Pre-train Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_config import ARTConfig, ARTEncoder_CLSConfig, SLTConfig\n",
    "\n",
    "art_config = ARTConfig(src_channel_size=30, tgt_channel_size=30, N=2)\n",
    "art_config.save_pretrained(\"test_config-art\")\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig(src_channel_size=30, tgt_channel_size=2, N=2)\n",
    "artcls_config.save_pretrained(\"artcls-config\")\n",
    "\n",
    "slt_config = SLTConfig(src_channel_size=30, tgt_channel_size=30, N=2)\n",
    "slt_config.save_pretrained(\"test_slt_confit\")\n",
    "print(slt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = ARTConfig.from_pretrained(\"test_config-art\")\n",
    "print(test_config)\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig.from_pretrained(\"artcls-config\")\n",
    "print(artcls_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Pre-train Model Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_model import ARTModel, ARTCLSModel, ART_CLS_PreTrain\n",
    "import torch\n",
    "\n",
    "art_model = ARTModel(test_config)\n",
    "cls_model = ARTCLSModel(artcls_config)\n",
    "cls_pretrain = ART_CLS_PreTrain(artcls_config)\n",
    "# resumeLoc = './ART/model/ART/modelsave/checkpoint.pth.tar'\n",
    "# # 2. load model\n",
    "# checkpoint = torch.load(resumeLoc)\n",
    "# art_model.model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# art_model.save_pretrained('test_config-art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel\n",
    "\n",
    "AutoConfig.register(\"ART\", ARTConfig)\n",
    "AutoModel.register(ARTConfig, ARTModel)\n",
    "\n",
    "AutoConfig.register(\"ARTEncoder_CLSConfig\", ARTEncoder_CLSConfig)\n",
    "AutoModel.register(ARTEncoder_CLSConfig, ARTCLSModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('test_config-art')\n",
    "# 加載目標模型\n",
    "target_model = AutoModel.from_pretrained('artcls-config')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract weight of Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# 加載來源模型\n",
    "source_model = AutoModel.from_pretrained('test_config-art')\n",
    "\n",
    "# 提取 Encoder 權重 (假設 Encoder 存在於 source_model.encoder 中)\n",
    "encoder_weights = source_model.model.encoder.state_dict()\n",
    "src_expandcov_weights = source_model.model.src_embed.state_dict()\n",
    "\n",
    "# 加載目標模型\n",
    "target_model = ARTCLSModel(artcls_config)\n",
    "\n",
    "# 將 Encoder 的權重加載到目標模型的 Encoder\n",
    "# 提取 Encoder 權重 (假設 Encoder 存在於 source_model.encoder 中)\n",
    "target_model.model.encoder.load_state_dict(encoder_weights)\n",
    "target_model.model.src_embed.load_state_dict(src_expandcov_weights)\n",
    "\n",
    "print(\"Encoder weights successfully transferred!\")\n",
    "target_model.save_pretrained('artcls-config')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================================================================================================\n",
      "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Param %                   Kernel Shape\n",
      "===============================================================================================================================================================================\n",
      "ART_AUG                                            [32, 30, 1024]            [32, 30, 1024]            592,512                     3.63%                   --\n",
      "├─Sequential: 1-1                                  [32, 30, 1024]            [32, 30, 128]             --                             --                   --\n",
      "│    └─MLP_projector: 2-1                          [32, 30, 1024]            [32, 30, 128]             --                             --                   --\n",
      "│    │    └─Sequential: 3-1                        [32, 30, 1024]            [32, 30, 2048]            2,099,260                  12.84%                   --\n",
      "│    │    └─Sequential: 3-2                        [32, 30, 2048]            [32, 30, 2048]            4,196,412                  25.68%                   --\n",
      "│    │    └─Sequential: 3-3                        [32, 30, 2048]            [32, 30, 128]             262,332                     1.61%                   --\n",
      "│    └─PositionalEncoding: 2-2                     [32, 30, 128]             [32, 30, 128]             --                             --                   --\n",
      "│    │    └─Dropout: 3-4                           [32, 30, 128]             [32, 30, 128]             --                             --                   --\n",
      "├─Encoder: 1-2                                     [32, 30, 128]             [32, 30, 128]             --                             --                   --\n",
      "│    └─ModuleList: 2-3                             --                        --                        --                             --                   --\n",
      "│    │    └─EncoderLayer: 3-5                      [32, 30, 128]             [32, 30, 128]             592,768                     3.63%                   --\n",
      "│    │    └─EncoderLayer: 3-6                      [32, 30, 128]             [32, 30, 128]             592,768                     3.63%                   --\n",
      "│    └─LayerNorm: 2-4                              [32, 30, 128]             [32, 30, 128]             256                         0.00%                   --\n",
      "├─Sequential: 1-3                                  [32, 30, 1024]            [32, 30, 128]             --                             --                   --\n",
      "│    └─MLP_projector: 2-5                          [32, 30, 1024]            [32, 30, 128]             --                             --                   --\n",
      "│    │    └─Sequential: 3-7                        [32, 30, 1024]            [32, 30, 2048]            2,099,260                  12.84%                   --\n",
      "│    │    └─Sequential: 3-8                        [32, 30, 2048]            [32, 30, 2048]            4,196,412                  25.68%                   --\n",
      "│    │    └─Sequential: 3-9                        [32, 30, 2048]            [32, 30, 128]             262,332                     1.61%                   --\n",
      "│    └─PositionalEncoding: 2-6                     [32, 30, 128]             [32, 30, 128]             --                             --                   --\n",
      "│    │    └─Dropout: 3-10                          [32, 30, 128]             [32, 30, 128]             --                             --                   --\n",
      "├─Decoder: 1-4                                     [32, 30, 128]             [32, 30, 128]             --                             --                   --\n",
      "│    └─ModuleList: 2-7                             --                        --                        --                             --                   --\n",
      "│    │    └─DecoderLayer: 3-11                     [32, 30, 128]             [32, 30, 128]             658,816                     4.03%                   --\n",
      "│    │    └─DecoderLayer: 3-12                     [32, 30, 128]             [32, 30, 128]             658,816                     4.03%                   --\n",
      "│    └─LayerNorm: 2-8                              [32, 30, 128]             [32, 30, 128]             256                         0.00%                   --\n",
      "├─Linear: 1-5                                      [32, 30, 128]             [32, 30, 1024]            132,096                     0.81%                   --\n",
      "===============================================================================================================================================================================\n",
      "Total params: 16,344,296\n",
      "Trainable params: 16,344,296\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.MEGABYTES): 504.01\n",
      "===============================================================================================================================================================================\n",
      "Input size (MB): 8.10\n",
      "Forward/backward pass size (MB): 239.86\n",
      "Params size (MB): 62.48\n",
      "Estimated Total Size (MB): 310.44\n",
      "===============================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from tf_model import ARTCLSModel, ARTModel, SLTModel, ART_AUG\n",
    "from tf_config import ARTConfig, SLTConfig\n",
    "from torchinfo import summary\n",
    "\n",
    "config = SLTConfig(sensor_time=1024, source_voxel_time=1024, d_model=128, tgt_channel_size=30, src_channel_size=30, N=2)\n",
    "\n",
    "model = ART_AUG(config)\n",
    "\n",
    "print(summary(model, input_size=[(32, 30, 1024),(32, 30, 1024),(32, 30, 30),(32, 30, 30)], col_names=[\"input_size\", \"output_size\", \"num_params\",  \"params_percent\", \"kernel_size\"]))\n",
    "\n",
    "# print(summary(cls_model, input_size=[(32, 30, 1024),(32,1024,1024)], col_names=[\"input_size\", \"output_size\", \"num_params\",  \"params_percent\", \"kernel_size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLTConfig {\n",
      "  \"N\": 4,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_model\": 128,\n",
      "  \"dropout\": 0.1,\n",
      "  \"h\": 8,\n",
      "  \"model_type\": \"SLT\",\n",
      "  \"sensor_time\": 100,\n",
      "  \"source_voxel_time\": 100,\n",
      "  \"src_channel_size\": 30,\n",
      "  \"tgt_channel_size\": 234,\n",
      "  \"transformers_version\": \"4.46.1\"\n",
      "}\n",
      "\n",
      "tensor(1.9999, device='cuda:0')\n",
      "tensor(1.9992, device='cuda:0')\n",
      "tensor(1.9996, device='cuda:0')\n",
      "tensor(2.0005, device='cuda:0')\n",
      "tensor(1.9994, device='cuda:0')\n",
      "Training Time =0.5111699104309082\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "\n",
    "from tf_config import ARTConfig, ARTEncoder_CLSConfig, SLTConfig\n",
    "from tf_model import ARTModel, ARTCLSModel, ART_CLS_PreTrain, SLTModel, SLTModel_ver2\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "slt_config = SLTConfig(src_channel_size=30, tgt_channel_size=234, N=4, source_voxel_time=100, sensor_time=100)\n",
    "slt_config.save_pretrained(\"test_slt_confit\")\n",
    "print(slt_config)\n",
    "\n",
    "slt_model = SLTModel(slt_config)\n",
    "\n",
    "# print(summary(slt_model, input_size=[(32, 30, 100),(32, 204, 100),(32,100,100),(32,100, 100)], col_names=[\"input_size\", \"output_size\", \"num_params\",  \"params_percent\", \"kernel_size\"]))\n",
    "\n",
    "# # 假設你的設備是 GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 模擬輸入數據\n",
    "src = torch.randn(256, 30, 100).to(device)  # shape: (32, 30, 1024)\n",
    "tgt = torch.randn(256, 234, 100).to(device)  # shape: (32, 30, 1024)\n",
    "labels = torch.randn(256, 204, 100).to(device)\n",
    "mask = torch.rand(256, 204) < 0.15  # (batch, 204)\n",
    "\n",
    "slt_model.to(device)\n",
    "mask = mask.to(device)\n",
    "\n",
    "# \"\"\" ART Test \"\"\"\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    for i in range(0, 5):\n",
    "        output = slt_model(src=src, tgt=tgt, src_mask=None, tgt_mask=None, tgt_token_mask=mask, \n",
    "                           labels=labels, return_dict = True)\n",
    "        print(output[\"loss\"])\n",
    "    end_time = time.time()\n",
    "    print(f\"Training Time ={end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    record_shapes=True,\n",
    "    profile_memory=True\n",
    ") as prof:\n",
    "    output = slt_model(src=src, tgt=src, src_mask=None, tgt_mask=None, labels=label, return_dict=True)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from torchinfo import summary\n",
    "\n",
    "from tf_config import ARTConfig, ARTEncoder_CLSConfig\n",
    "from tf_model import ARTModel, ARTCLSModel, ART_CLS_PreTrain\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "art_config = ARTConfig(src_channel_size=30, tgt_channel_size=30, N=2)\n",
    "art_config.save_pretrained(\"test_config-art\")\n",
    "\n",
    "artcls_config = ARTEncoder_CLSConfig(src_channel_size=30, tgt_channel_size=2, N=2)\n",
    "artcls_config.save_pretrained(\"artcls-config\")\n",
    "\n",
    "test_config = ARTConfig.from_pretrained(\"test_config-art\")\n",
    "artcls_config = ARTEncoder_CLSConfig.from_pretrained(\"artcls-config\")\n",
    "# print(test_config)\n",
    "# print(artcls_config)\n",
    "\n",
    "art_model = ARTModel(test_config)\n",
    "cls_model = ARTCLSModel(artcls_config)\n",
    "cls_pretrain = ART_CLS_PreTrain(artcls_config)\n",
    "\n",
    "# 模擬輸入數據\n",
    "src = torch.randn(32, 30, 1024)  # shape: (32, 30, 1024)\n",
    "src_mask = torch.randn(32, 1024, 1024)  # shape: (32, 1024, 1024)\n",
    "# label = torch.randint(0, 2, (32,))\n",
    "label = torch.randn(32, 30, 1024)\n",
    "\n",
    "# 假設你的設備是 GPU\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 移動數據到 GPU\n",
    "src = src.to(device)\n",
    "src_mask = src_mask.to(device)\n",
    "cls_pretrain = cls_pretrain.to(device)\n",
    "label = label.to(device)\n",
    "\n",
    "\"\"\" ART Classifier Test \"\"\"\n",
    "# output = cls_model(src, None, return_dict = True)\n",
    "# logits = output.last_hidden_state.squeeze(dim=1)  # shape: [32, 2]\n",
    "# print(output.last_hidden_state.shape)\n",
    "# loss_fct = CrossEntropyLoss()\n",
    "# loss = loss_fct(logits, label)\n",
    "# print(loss)\n",
    "\n",
    "# loss = cls_pretrain(src, None, label)\n",
    "# print(loss.loss)\n",
    "\n",
    "\"\"\" ART Test \"\"\"\n",
    "output = art_model(src=src, tgt=src, src_mask=None, tgt_mask=None, labels=label, return_dict = True)\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulate CLS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 模擬自定義 ART Classifier Dataset\n",
    "class MockDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, input_dim, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # 隨機生成數據\n",
    "        self.data = torch.randn(num_samples, seq_len, input_dim)  # 模擬 src\n",
    "        self.masks = torch.randn(num_samples, input_dim, input_dim)  # 模擬 src_mask\n",
    "        self.labels = torch.randint(0, num_classes, (num_samples,))  # 模擬 label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src\": self.data[idx], \n",
    "            \"src_mask\": self.masks[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# 模擬數據集參數\n",
    "train_dataset = MockDataset(num_samples=1000, seq_len=30, input_dim=1024, num_classes=2)\n",
    "eval_dataset = MockDataset(num_samples=200, seq_len=30, input_dim=1024, num_classes=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulate ART datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 模擬自定義 ART Model Dataset\n",
    "class MockDataset(Dataset):\n",
    "    def __init__(self, num_samples, seq_len, input_dim):\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # 隨機生成數據\n",
    "        self.data = torch.randn(num_samples, seq_len, input_dim)  # 模擬 src\n",
    "        self.masks = torch.randn(num_samples, input_dim, input_dim)  # 模擬 src_mask\n",
    "        self.labels = torch.randn(num_samples, seq_len, input_dim)  # 模擬 label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src\": self.data[idx], \n",
    "            \"tgt\": self.data[idx], \n",
    "            \"src_mask\": self.masks[idx],\n",
    "            \"tgt_mask\": self.masks[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# 模擬數據集參數\n",
    "train_dataset = MockDataset(num_samples=16, seq_len=30, input_dim=1024)\n",
    "eval_dataset = MockDataset(num_samples=16, seq_len=30, input_dim=1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 檢查數據集中的一個樣本\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample src shape:\", sample[\"src\"].shape)       # (30, 1024)\n",
    "print(\"Sample src_mask shape:\", sample[\"src_mask\"].shape)  # (1024, 1024)\n",
    "print(\"Sample label:\", sample[\"label\"])              # 標籤值\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ART Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, './FirstMultiModel/EEGART')\n",
    "from torchinfo import summary\n",
    "\n",
    "from tf_config import ARTConfig, ARTEncoder_CLSConfig\n",
    "from tf_model import ARTModel, ARTCLSModel, ART_CLS_PreTrain\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# art_config = ARTConfig(src_channel_size=30, tgt_channel_size=30, N=2)\n",
    "# art_config.save_pretrained(\"test_config-art\")\n",
    "\n",
    "# artcls_config = ARTEncoder_CLSConfig(src_channel_size=30, tgt_channel_size=2, N=2)\n",
    "# artcls_config.save_pretrained(\"artcls-config\")\n",
    "\n",
    "test_config = ARTConfig.from_pretrained(\"test_config-art\")\n",
    "\n",
    "art_model = ARTModel(test_config)\n",
    "\n",
    "# 自定义数据整理器\n",
    "class SignalDataCollator:\n",
    "    def __call__(self, features):\n",
    "        inputs = torch.stack([f[\"src\"] for f in features])\n",
    "        masks  = torch.stack([f[\"src_mask\"] for f in features])\n",
    "        labels = torch.stack([f[\"label\"] for f in features])\n",
    "        return_dict = True\n",
    "        return {\"src\": inputs, \n",
    "                \"tgt\":inputs, \n",
    "                \"src_mask\": masks, \n",
    "                \"tgt_mask\": masks, \n",
    "                \"labels\": labels, \n",
    "                \"return_dict\": return_dict}\n",
    "\n",
    "\n",
    "# 自定义评价指标\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, targets = eval_preds\n",
    "    mse = ((predictions - targets) ** 2).mean()\n",
    "    return {\"mse\": mse}\n",
    "\n",
    "# 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "# 初始化模型和 Trainer\n",
    "trainer = Trainer(\n",
    "    model=art_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=SignalDataCollator(),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLS Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate \n",
    "\n",
    "# metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     logits, labels = eval_preds\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",       # 儲存模型的目錄\n",
    "    eval_strategy=\"epoch\",  # 替换 evaluation_strategy\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    # save_strategy=\"epoch\",       # 每個 epoch 保存一次模型\n",
    "    # logging_dir=\"./logs\",        # 日誌目錄\n",
    "    # logging_steps=10,\n",
    ")\n",
    "# training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "\n",
    "# 創建 Trainer\n",
    "trainer = Trainer(\n",
    "    model=art_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
